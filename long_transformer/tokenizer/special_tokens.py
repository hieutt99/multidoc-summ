class SpecialTokens():
    sep_token = '[SEP]'
    cls_token = '[CLS]'
    pad_token = '[PAD]'
    tgt_bos = '[unused1]' # 0
    tgt_eos = '[unused2]' # 1
    # tgt_sent_split = '[unused3]' # 2
    # src_story_split = '[unused4]' # 3
    # vocab = tokenizer.get_vocab()
    # sep_vid = vocab[sep_token]
    # cls_vid = vocab[cls_token]
    # pad_vid = vocab[pad_token]